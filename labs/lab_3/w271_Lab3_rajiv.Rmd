---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Group Lab 3'
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

## Instructions (Please Read Carefully):

* $\textbf{Due 4pm Tuesday August 11 2020}$

* 20 page limit (strict)

* Do not modify fontsize, margin or line-spacing settings

* One student from each group should submit the lab to their student github repo by the deadline; submission and revisions made after the deadline will not be graded

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers. Do not suppress the codes in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members names. For example the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab3.Rmd`
    * `StanCartman_KennyKyle_Lab3.pdf`
            
* Although it sounds obvious, please write your names on page 1 of your pdf and Rmd files

* For statistical methods that we cover in this course, use the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you must provide an explanation of why such libraries and functions are used and reference the library documentation. For data wrangling and data visualization, you are free to use other libraries, such as `dplyr`, `ggplot2`, etc.

* Your report needs to include:

    * A thorough analysis of the given dataset, which includ examiniation of anomalies, missing values, potential of top and/or bottom code, and other potential anomalies, in each of the variables.
        
    * A comprehensive Exploratory Data Analysis (EDA) analysis, which includes both graphical and tabular analysis, as taught in this course. Output-dump (that is, graphs and tables that don't come with explanations) will result in a very low, if not zero, score. Be selective when choosing visuals and tables to illustrate your key points and concise with your explanations (please do not ramble). 
    
    * A proper narrative for each question answered. Make sure that your audience can easily follow the logic of your analysis and the rationale of decisions made in your modeling, supported by empirical evidence. Use the insights generated from your EDA step to guide your modeling approach.
    
    * Clear explanations of all steps used to arrive at a final model, with conclusions that summarize results with respect to the question(s) being asked and key takeaways from the analysis.

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file.

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity

\newpage

```{r message=FALSE}
library(foreign)
library(gplots)
library(ggplot2)
library(stats)
library(Hmisc)
library(car)
library(usmap)
library(dplyr)
library(gridExtra)
library(stargazer)
library(reshape2)
library(data.table)
library(tidyr)
library(grid)
library(plm)
#library(Rmisc) # for arranging plots
```
# U.S. traffic fatalities: 1980-2004

In this lab, you are asked to answer the question **"Do changes in traffic laws affect traffic fatalities?"**  To do so, you will conduct the tasks specified below using the data set *driving.Rdata*, which includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is come with the dataste.

**Exercises:**

# Part 1

1. (30%) Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*


This dataset is at an annual level - one row per state, per year for the 48 contiiguous states from 1980 to 2004. There are no missing values in the dataset.


```{r fig.width=8, fig.height=8, warning=FALSE}

load("driving.RData")

# one row per year per state
head(table(data$year, data$state))
min(data$year)
max(data$year)

# merge with state codes
fips_map <-read.csv("statecodes.csv")
d_data <- merge(x=data, y=fips_map, by="state",all.x = TRUE) %>% dplyr::select(-state)
d_data <- dplyr::rename(d_data, c("state"="code"))

d_data <- data.table(d_data)

# Zero missing values across all columns
d_data[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = 1:56]

# average fatality rate per 100,000 across states
state_avg <- d_data %>% group_by(state) %>% summarise(avg_totfatrte=mean(totfatrte), .groups = 'drop')

state_avg <- dplyr::rename(state_avg, c("value"="avg_totfatrte"))

p1.1 <- plot_usmap(data = state_avg,  values="value", color = "red") + 
  scale_fill_continuous(name="", low="white", high="red") + 
  theme(legend.position = "right") +ggtitle("Average fatality rate per 100,000 (1980-2004)")

# average fatality rate per 100,000 across years
year_avg <- d_data %>% group_by(year) %>% summarise(avg_totfatrte=mean(totfatrte))

p1.2 <- ggplot(data=year_avg, aes(x=year, y=avg_totfatrte)) +
  geom_line()+ ggtitle("Average fatality rate across US per 100,000")

grid.arrange(p1.1, p1.2, nrow=2)


```

## Growth Gurve Analysis
- Note general flat to downward trend with exception of Mississippi
- Nevada and New Mexico drop looks steep


```{r fig.width=8, fig.height=8}

xyplot(totfatrte~year | state, data=d_data, 
       prepanel = function(x, y) prepanel.loess(x, y, family="gaussian"),
       xlab = "Year", ylab = "Avg. Fatality Rate",
       panel = function(x, y) {
          panel.xyplot(x, y)
          panel.loess(x,y, family="gaussian") },
       as.table=T)



# this is hard to read!
#g <- ggplot(data_state, aes(year, totfatrte, colour = as.factor(code)))
#g + geom_line() + ggtitle("Growth Curve by state")

```

## Investigation of laws that changed over time

We note that the laws are not binary Variables. *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl and perc14_24* are yes-no indicator dummies, with the caveat that they can be fractional as noted in the problem statement. The fractional values will ideally need to be changed to 0 or 1 for *plotting purposes only* based on whether the variable is < 0.5 or >= 0.5. Note that we need to do special handling of the edge case of 0.5 in two categories. For the regression models, we use the original data as is since we do not see any value in dropping the information contained.


```{r warning=FALSE}

## generic plot function for stacked charts
genplot <- function(df, col, legend){
  df2 <- table(df[[col]], df$year) %>% reshape2::melt()
  colnames(df2) <- c(col, 'Year', 'NumberOfStates')
  df2.plot <- ggplot(df2, aes(fill=factor(get(col)), y=NumberOfStates, x=Year))+
                geom_bar(position="stack", stat = "identity") +
              guides(fill=guide_legend(title=legend)) 
#              scale_fill_discrete(palette=scales::hue_pal())
            
  
  return (df2.plot)
}
```


The stacked bar charts below show how the laws changed over time across the states. 
* We note that the alcohol related laws get more conservative over the years. Only about 15 states had *blood alcohol limit* laws in 1980. We see that proportion increase to about 40 states in the mid 80's with some states beginning to adopt the stricter limit of BAC08. More states also adopt the *Per Se law* that makes violating the BAC limit while driving on its own an offence. 
* We note the variation in the *minimum drinking age* among states in the early 80s with different states having different age limits. However by the late 80s all states conform to the minimum drinking age of 21. The *Zero tolerance law* that specifically targets youth drinking starts to get introduced in the mid 80s and we see a sharp increase in the number of states adopting the law over the next decade to full compliance.
* The *Graduated driver license* law that alows young drivers to gain safe driving experience gets introduced in the early 90s with more states adopting it over the next decade. 
* We see a < 10 states beginning to adopt the *primary seatbelt law* in the mid 80s and there seems to be a gradual increase to about 20 states in the mid 2000's. The stricter secondary seatbelt law has a much more steep increase and then a gradual decline. Since the interpretation of the law in different states are different its a bit hard to determine the drop in adoption in the mid 90s.
* We see a general trend of *Speed limits* increasing over time across the US starting in the late 80s. This is possibly due to the cars in general getting faster and roads getting better.


```{r}
# Speed Limits

d_data[sl55==0.5 & sl65==0.5, c("sl55", "sl65"):=list(0,1) ]
d_data[sl65==0.5 & sl70==0.5, c("sl65", "sl70"):=list(0,1) ]
d_data[sl65==0.5 & sl75==0.5, c("sl65", "sl75"):=list(0,1) ]

for (sp in c("sl55","sl65","sl70","sl75", "slnone")){
  d_data[get(sp) >0.5, eval(quote(sp)):= 1]
  d_data[get(sp) <0.5, eval(quote(sp)):= 0]
}

d_data.speed <- d_data %>%
  gather(key="SpeedLimit", value="Value", "sl55","sl65","sl70","sl75", "slnone") %>% 
  dplyr::filter(Value==1) %>%
  dplyr::select(-Value) %>% data.table()

d_data.speed[, SpeedLimit:=factor(SpeedLimit)]

speed.plot <- genplot(d_data.speed, "SpeedLimit", "SpeedLimit")

# merge bac10 and bac08 to one
d_data[bac10==0.5 & bac08==0.5, c("bac10", "bac08"):=list(0,1) ]

d_data.bac <- d_data %>%
  gather(key="BAC", value="Value", "bac10","bac08") %>% 
  dplyr::filter(Value==1) %>%
  dplyr::select(-Value) %>% data.table()

d_data.bac$BAC <- factor(d_data.bac$BAC)


bac.plot <- genplot(d_data.bac, "BAC", "BAC")


for (sp in c("zerotol","gdl", "perse", "sbprim", "sbsecon", "sl70plus")){
  d_data[get(sp) >0.5, eval(quote(sp)):= as.integer(1)]
  d_data[get(sp) <=0.5, eval(quote(sp)):= as.integer(0)]

}

# minage needs to be rounded
d_data[,minage:=round(minage)]

minage.plot <- genplot(d_data, "minage", "Minimum\nDrinking\nAge")
zerotol.plot <- genplot(d_data, "zerotol", "Zero\nTolerance\nLaw")
bac.plot <- genplot(d_data.bac, "BAC", "BAC")
perse.plot <- genplot(d_data, "perse", "Per\nSe\nLaw")

grid.arrange(zerotol.plot, minage.plot, bac.plot,perse.plot, top=textGrob("Alcohol related laws",gp=gpar(fontsize=12,font=3)))


# License laws
gdl.plot <- genplot(d_data, "gdl", "Grad.\nDrivers\nLicense\nLaw")

# seatbelt laws
sbprim.plot <- genplot(d_data, "sbprim", "Primary\nSeatbelt\nLaw")
sbsecon.plot <- genplot(d_data, "sbsecon",  "Secondary\nSeatbelt\nLaw")

#grid.arrange(gdl.plot, sbprim.plot, sbsecon.plot)
grid.arrange(gdl.plot, sbprim.plot, sbsecon.plot,speed.plot, top=textGrob("Other laws",gp=gpar(fontsize=12,font=3)))



```

# Part 2

2. (15%) How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.



## What is the average of this variable in each of the years in the time period covered in this dataset

```{r warning=FALSE}

# yearly average nationwide
year_avg

```

## Regression model and explanation

This model gives us the time effect on fatality rate. The intercept in this case is the average *totfatrte* across all states in the omitted year 2004. Each of the coefficients `d80`, `d81`...`d04` is the average increase in *totfatrte* relative to the base year 2004. 

```{r warning=FALSE}

#stargazer(lm(totfatrte~d80+d81+d82+d83+d84+d85+d86+d87+d88+d89+d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+d00+d01+d02+d03, data=data), type = "text")

stargazer(lm(totfatrte~factor(year), data=d_data), type = "text")


```

## Did driving become safer

This model highlights that driving has become safer between the years 1982 through 2004 with respect to the base year 1980 since the differences are significant at the < 0.05.

# Part 3

3. (15%) Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)

As seen earlier, variables `bac08`, `bac10`, `perse`, `sbprim`, `sbsecon`, `sl70plus`, `gdl` are fractional values between 0 and 1. We use the original values as is so as not to lose any information. Variables `unem`, `perc14_24` and  `vehiclesmilespc` are continuous. We could log transform `vehiclesmilespc` only if we want to interpret the coefficients in terms of percentage changes.



```{r warning=FALSE}

model.3 <- lm(totfatrte~bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+perc14_24+unem+vehicmilespc+factor(year), data=data)

stargazer(model.3, type="text")


```

### Check model assumptions

We see some evidence of heteroskedasticity pointing to omitted variables.

```{r fig.width=8, fig.height=6}
par(mfrow=c(2,2))
plot(model.3)

```
### Do *per se laws* have a negative effect on the fatality rate?
From the model output above, it is evident that per se laws have had a negative effect on the fatality rate, as seen from the coefficient of -0.756 and p value < 0.05

### What about having a primary seat belt law? 
We do not see evidence of the primary seat belt law having any effect on the fatality rate as seen from the p value. This seems suspicous and points to the limitations inherent in pooled OLS models.


# Part 4

4. (15%) Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

The Pooled OLS model from Part 3 does not consider heterogeniety across each state. Each state has state specific factors that influence the fatality rate over time and the Pooled OLS model does not differentiate between the state specific differences over time and therefore treats all the observations the same way. Therefore the fixed effects model is more reliable for this inference. The output of the model produces different estimates from the pooled OLS model. We note that the model correctly shows the statistically significant negative effect of `sbprim` on the dependent variable.


```{r warning=FALSE}
d_data2 <- pdata.frame(d_data, c("state", "year"))
model.4 <-
  plm(
    totfatrte ~ bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+perc14_24+unem+vehicmilespc+factor(year),
    model = "within",
    data = d_data2
  )

stargazer(model.4, type="text")
```

### Model assumtions
Our *Pooled OLS model* has the form. Here $y_{81}$...$y_{04}$ are dummy variables that represent each year. $a_i$ and $u_{it}$ together form the composite error term. $a_i$ is the unobserved state effect. This represents all factors that affecting fatality rates that do not change over time, such as population demographics which are slow to change. $u_{it}$ is the time varying error. One of the assumptions of the pooled OLS model is that the composite error term is uncorrelated with the explanatory variables, which does not make sense in this case since there could be state specific factors that effect the fatality rate. Pooled OLS is therefore biased and inconsistent if the explanatory variables $bac_{08}$, $bac_{10}$... $\beta_2vehicmilespc$ are correlated to $a_i$

$$
\begin{aligned}\\
totfatrte = \beta_o + \beta_1bac08+\beta_2bac10+\beta_3perse+\beta_4sbprim+ \\
\beta_5sbsecon+\beta_6sl70plus+\beta_7gdl+\beta_28perc14\_24+\\
\beta_9unem+\beta_10vehicmilespc+ \\
\delta_oy_{81}+\delta_{1}y_{82}+...+\delta_{23}y_{04} + a_i+u_{it}, \\t=82, 83... 04
\end{aligned}
$$


The *Fixed effect model* has the form seen below. Here we see that the state specific unobserved effect a_i has disappeared. Therefore we note that the fixed effects model allows for arbitrary correlation between $a_i$ and all the explanatory variables since any explanatory variables that is constant over time gets swept away by the fixed effects transformation. The other assumptions for this fixed effects model is that errors $u_{it}$ are homoskedastic and serially uncorrelated across $t$

$$
\begin{aligned}\\
totfatrte_{it}-\hat{totfatrte_{it}}= \beta_1(bac08-\hat{bac08})+...+
\beta_2(vehicmilespc-\hat{vehicmilespc})+ u_{it}, \\t=82, 83... 04
\end{aligned}
$$



# Part 5

5. (10%) Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

The null hypothesis of the Hausman test is the Random effects is preferred, the alternate hypothesis is that the fixed effects model is preferred. The below result suggests that the fixed effect model is preferred. 


```{r}
model.re <-   plm(
    totfatrte ~ bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+perc14_24+unem+vehicmilespc+factor(year),
    index = c("state"),
    model = "random",
    data = d_data
  )
phtest(model.4, model.re)
```
# Part 6

6. (10%) Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

The estimated effect of 1 mile increase per capita is 0.001 increase in `totfatrte`. Therfore a 1,000 mile increase per capita would result in a corresponding 1% increase in fatality rate. The 95% confidence interval of the estimate is $1.0\pm1.96*(1000*0.0001)$ which equals (0.804,1.196)

#Part 7

7. (5%) If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?













