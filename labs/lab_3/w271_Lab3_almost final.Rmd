---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Group Lab 3'
geometry: margin=1in
output:
  pdf_document:
  toc: yes
author: Rajiv Nair, Atit Wongnophadol, Julia Ying
fontsize: 11pt
---

# U.S. traffic fatalities: 1980-2004

# Part 1

1. (30%) Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*


```{r message=FALSE, include = FALSE}
library(foreign)
library(gplots)
library(ggplot2)
library(stats)
library(Hmisc)
library(car)
library(usmap)
library(dplyr)
library(gridExtra)
library(stargazer)
library(reshape2)
library(data.table)
library(tidyr)
library(grid)
library(plm)
library(lmtest)
library(knitr)
```

There are 1,200 observations and 56 variables in this dataset. The panel dataset has two indices, `year` and `state`. The 48 continental states, numbered based on the alphabetical order, each have one row of data per year, spanning 25 years from 1980-2004, with no missing data in the dataset. There are nine traffic fatality rate measures, measuring total, weekend, and nighttime fatality count, fatality per 100,000 population, and fatality per 100 million miles. The fatality rate per 100,000 population, `totfatrte`, is the outcome variable of interest in this study. Figure \ref{fig:rateuni} shows the univariate EDA on `totfatrte`. `totfatrte` is asymmetrically distributed with a positive skew, ranging from 6.2 to 53.32 with a median of 18.92. On a state level, New Mexico, Wyoming and Mississippi have the highest total fatality rate averaged over 25 years, while New York, New Jersey and Massachusetts have the lowest averaged total fatality rate. The year to year fatality rate have small fluctuations in variance. In general, fatality rate decreases over time, with the exception of a small increase around 1987.

Figure \ref{fig:rategrowth} displays the total fatality rate over time for each of the 48 states. Most of the states have declining or steady fatality rate, with New Mexico, Nevada and Montana having the largest declines. Exceptions to this pattern were Wyoming, which shows a u-shaped pattern for fatality rate, and Mississippi, which has a slight increasing trend. Both of these states are among the states with highest average fatality rate.


```{r warning=FALSE, message = FALSE, fig.cap="\\label{fig:rateuni}Univariate Analysis of Total Fatality Rate"}
load("driving.RData")

paste(length(unique(data$year)), min(data$year), max(data$year), unique(table(data$year)))
paste(length(unique(data$state)), unique(table(data$state)))
unique(table(data$year, data$state))

summary(data$totfatrte)

# merge with state codes
alpha_map <-read.csv("statecodes.csv")
data <- merge(x=data, y=alpha_map, by="state",all.x = TRUE) %>% dplyr::select(-state)
data <- dplyr::rename(data, c("state"="code")) 

state_avg <- data %>% group_by(state) %>% summarise(avg_totfatrte=mean(totfatrte), 
                                                    .groups = 'drop')

# format options for combined plots
formatting <- theme(plot.title = element_text(hjust = 0.5, size=10), 
               axis.title.x = element_text(size = 10), 
               axis.title.y = element_text(size = 10),
               legend.title = element_text(size = 10))

tickformat <- theme(legend.position = "bottom", axis.text.y = element_text(size = 8), 
                    axis.text.x = element_text(angle=90, vjust = 0.5, hjust=1, size = 7))

# functions for making histograms, boxplots and scatterplots
plotbox <- function (df, ycol, ylab){
          ggplot(data, aes(factor(year), get(ycol))) +  
                          geom_boxplot() + 
                          labs(y=ylab, x = "Year") + 
                          formatting + tickformat +
                          scale_x_discrete(breaks=seq(1980, 2004, 2))}

plothist <- function(df, xcol, xlab){
  ggplot(df, aes(x=get(xcol)))+ 
      geom_histogram(color="black", fill = "white")+
      labs(y='Count', x =xlab)+
      formatting + 
      geom_vline(aes(xintercept=mean(get(xcol))),
            color="blue", linetype="dashed", size=1)
}

plotscatter <- function(df, xcol, xlab){
  ggplot(df, aes(x=get(xcol), y=totfatrte)) + geom_point()+
  geom_smooth(method=lm, se=FALSE) + ggtitle(paste(xcol,' vs tolfatrte'))+ 
  xlab(xlab) + ylab('Fatality Per 100,000') + formatting
}

p1 <- plot_usmap(data = state_avg,  values="avg_totfatrte", color = "red") + 
  scale_fill_continuous(name="Per 100,000", low="white", high="red") + 
  theme(legend.position = "right") +ggtitle("Average Fatality Rate from 1980 to 2004 by State")

p2 <- plothist(data, "totfatrte", "Fatality per 100,000") +  ggtitle("Total Fatality Rate Distribution")
p3 <- plotbox(data, "totfatrte", "Fatality per 100,000") +  ggtitle("Boxplots of Total Fatality Rate by Year")

year_avg <- data %>% group_by(year) %>% summarise(avg_totfatrte=mean(totfatrte))
p4 <- ggplot(data=year_avg, aes(x=year, y=avg_totfatrte)) +
  geom_line()+ ggtitle("Average Fatality Rate by Year") +
  xlab('Year') + ylab('Fatality per 100,000')+ formatting


grid.arrange(p1,p2,p3,p4, nrow = 2)
```

```{r fig.width=6, fig.height=6, fig.cap="\\label{fig:rategrowth}Total Fatality Rate by State"}
xyplot(totfatrte~year | state, data=data, 
       prepanel = function(x, y) prepanel.loess(x, y, family="gaussian"),
       xlab = "Year", ylab = "Fatality Rate per 100,000 Population",
       panel = function(x, y) {
          panel.xyplot(x, y)
          panel.loess(x,y, family="gaussian") },
       as.table=T)
```


In addition to the indices and fatality measures, the panel data set contains 25 dummy variables representing each year in the dataset. The remaining columns are potential explanatory variables. State population size `statepop` is not pertinent to this study since fatality rate is per 100,000 people. There are four variables not directly related to traffic laws. Minimum drinking age (`minage`) ranges from 18 to 21 in the panel dataset. Starting in 1989, all states require minimal age of 21 to drink, so for most of the data set, `minage` is time invariant, and it is not included in the regression.

Figure \ref{fig:nontrafficeda} shows the univariate and bivariate EDA of the other three variables, `perc14_24`, `unem` and `vehicmilespc`. Percent population aged 14 through 24 (`perc14_24`) ranges from 11.7 to 20.30 with median of 14.9. The percentage decreases steadily until 1990, and then remains steady. Its variance is higher in the 90s, and every years since 1990 has at least one high outlier. Unemployment Rate Percentage (`unem`) ranges from 2.2 to 18% with median of 5.6%. `unem` fluctuates over time with a slight decreasing trend, and its variance reduces over time. Vehicle Miles per Capita (`vehicmilespc`) range from 4372 to 18390 miles with a median of 9013 miles, and it is increasing over time with increasing variance. Summary statistic and histogram suggest all three are asymmetrically distributed with a positive skew, and scatter plots show all three have weakly positive contemporaneous correlation to total fatality, with `perc_14_24` having the strongest correlation among the three.

```{r message = FALSE, fig.width=8, fig.height=6, fig.cap="\\label{fig:nontrafficeda}Univariate and Bivariate EDA of Factors Unrelated to Traffic Laws"}
# get class for columns besides year dummy and fatality
data %>% select(matches('^[^d]')) %>% select(-contains("fat")) %>% sapply(class)

summary(data[, c("minage", "perc14_24", "unem", "vehicmilespc")])
data %>% group_by(year) %>% summarise(avg_min_age=mean(minage))

p1a <- plotbox(data, "perc14_24", "Population Age 14-24(%)")
p2a <- plotbox(data, "unem", "Unemployment (%)")
p3a <- plotbox(data, "vehicmilespc", "Miles Per Capita")

p1b <- plothist(data, "perc14_24", "Population Age 14-24(%)")
p2b <- plothist(data, "unem", "Unemployment (%)")
p3b <- plothist(data, "vehicmilespc", "Miles Per Capita")

p1c <- plotscatter(data, "perc14_24", "Population Age 14-24(%)")
p2c <- plotscatter(data, "unem", "Unemployment (%)")
p3c <- plotscatter(data, "vehicmilespc", "Miles Per Capita")

grid.arrange(p1a, p1b, p1c, p2a, p2b, p2c, p3a, p3b, p3c, ncol = 3)
```

There are six variables corresponding to speed limit laws, one each for 55, 65, 70, 75 mph limit, one for no speed limit, and one for speed limit 70 and over or no limit. These are not binary indicator variables. They contain decimal values to indicate proportion of the year when a law is in effect in the event a law is enabled in the middle of a year. Figure \ref{fig:sl} shows the number of states enforcing each speed limit over time, with yearly averaged fatality rate superimposed. When there are two different speed limits in a year, the speed limit in-effect for majority of the year is considered the speed limit for the purpose of the plot. In the special case where it's a 50-50 split, the higher speed limit is used. In general, speed limit is increasing over time across the US starting in the late 80s. This is possibly due to the cars in generally getting faster and roads getting better. It's worth noting that the small peak in fatality rate in the late 80s coincides with the first two years of speed limit increase. It's possible that driving was more dangerous in the initial years while people adapted to the faster speed limit. For the regression, speed limit over 70 mph or no limit (`sl70plus`) is used as the explanatory variable.

```{r fig.width=5, fig.height=3, fig.cap="\\label{fig:sl}Speed Limit Distribution Over Time with Total Fatality Rate Superimposed"}
## generic plot function for stacked charts
genplot <- function(grouped_df, legend){
  names(grouped_df) <- c("Year", "condition", "n")
  bar.plot <- ggplot()+ geom_bar(data = grouped_df, aes(fill=condition, y=n, x=Year), 
              position="stack", stat = "identity") + geom_line(data = year_avg,
              aes(x = year, y = avg_totfatrte * 1.5), colour = "blue") + formatting +
              scale_y_continuous(sec.axis = sec_axis(trans = ~ . / 1.5, name = "Average Fatality Per 100,000")) + 
              guides(fill=guide_legend(title=legend)) +ylab("Number of States")
  return (bar.plot)
}

data <- data %>%
    mutate(sl = case_when(
      sl55 > 0.5 ~ "sl55",
      sl65 > 0.5 ~ "sl65",
      sl70 > 0.5 ~ "sl70",
      sl75 > 0.5 ~ "sl75",
      slnone > 0.5 ~ "slnone",
      sl55 == 0.5 & sl65 == 0.5 ~ "sl65",
      sl65 == 0.5 & sl70== 0.5 ~ "sl70",
      sl65 == 0.5 & sl75== 0.5 ~ "sl75"
    ))


data %>% group_by(year, sl) %>% tally() %>% genplot("Speed Limit")
```

Laws pertaining alcohol are blood alcohol limit at .10 (`bac10`),  blood alcohol limit at .08 (`bac8`), and zero tolerance (`zerotol`). `bac10` and `bac08` are rounded off to binary values depending on which law is in effect majority of the year and combined for the barchart. In case of a tie, the data point is considered having BAC limit of 0.1 in effect. Initially, majority of the states did not have a blood alcohol restriction. As seen in Figure \ref{fig:alcohol}, starting in mid 80s, most states adopted a blood alcohol limit of 0.1 or lower, and in the later years, increasing number of states adopted the more restrictive limit of 0.08. The initial increase in BAC restriction correlates to the initial sharp drop in fatality rate in the early 80s. The increase in imposing BAC limit of 0.08 loosely corresponds to the second drop in fatality rate in the late 80s and early 90s. The zero-tolerance law was non-existent in the beginning of the dataset. It was first introduced in 1983, and the number of states implementing the law dramatically increased in the 90s. The increase of zero-tolerance is correlated to the decrease of fatality rate.

```{r fig.width=10, fig.height=3, fig.cap="\\label{fig:alchol}Enforcement of Laws Regarding Alcohol with Total Fatality Rate Superimposed"}
data <- data %>%
    mutate(bac = case_when(
      bac10 > 0.5 ~ "BAC 0.1",
      bac08 > 0.5 ~ "BAC 0.08",
      bac08 == 0.5 & bac10== 0.5 ~ "BAC 0.1",
    ), zerotol_bin = case_when(zerotol > 0.5 ~ "Enforced")
    )

p1 <- data[!is.na(data$bac),] %>% group_by(year, bac) %>% tally() %>% genplot("Blood Alcohol")
p2 <- data[!is.na(data$zerotol_bin),]%>% group_by(year, zerotol_bin) %>% tally() %>% genplot("Zero-Tolerance")

grid.arrange(p1,p2, nrow = 1)
```


`seatbelt` indicates seatbelt laws, possible values are `primary`, `secondary`, or `none`. `sbprim` and `sbsecond` are binary indicator variables representing the same information. Other variables concerning traffic laws are graduated drivers license law (`gdl`),   administrative license revocation (`perse`). Similar to the speed limit variables, `gdl` and `perse` also contain decimals to represent laws in effect for parts of a year. They were also binarized for the bargraph visualizations (see Figure \ref{fig:traffic}). Seat belt laws were first implemented in 1985 and saw near total adoption by 1995. Increasing number of states adopted primary seatbelt laws starting mid 90s. Overall, seatbelt laws is inversely correlated to fatality rate, though due to its absence in early 80s, it did not contribute to the initial decrease in fatality. Graduated Driver License Law first began in 1996 and dramatically increased in enforcement over the next decade. By mid 90s, the fatality rate was already steady, so `gdl` does not have an obvious impact on fatality. Per Se Law became increasingly common starting early 80s, and is inversely correlated to fatality rate.

```{r fig.width=10, fig.height=3, fig.cap="\\label{fig:traffic}Enforcement of Traffic Laws with Total Fatality Rate Superimposed"}
data <- data %>%
    mutate(seatbelt_bin = case_when(seatbelt == 1 ~ "Primary",seatbelt == 2 ~ "Secondary")
      , gdl_bin =  case_when(gdl > 0.5 ~ "Enforced"),
    perse_bin = case_when(perse > 0.5 ~ "Enforced"))

p1 <- data[!is.na(data$seatbelt_bin),] %>% group_by(year, as.factor(seatbelt_bin)) %>% tally() %>% genplot("Seatbelt Law") + theme(legend.position="bottom")
p2 <- data[!is.na(data$gdl_bin),] %>% group_by(year, gdl_bin) %>% tally() %>% genplot("Graduated Driver\nLicense Law")+ theme(legend.position="bottom")
p3 <- data[!is.na(data$perse_bin),] %>% group_by(year, perse_bin) %>% tally() %>% genplot("Perse Law")+ theme(legend.position="bottom")

grid.arrange(p1,p2,p3, nrow = 1)
```

Finally, correlation plot (Figure \ref{fig:corrplot}) provides a cursory look at the relationship between the predictors and the outcome variable `totfatrte`. None of the predictors have exceptionally high correlation with fatality rate. `sl70plus`, `vehicmilespc`, `perc14_24`, `unem` have positive contemporaneous correlation to the outcome variable, and the rest have negative correlation. As previously noted in the bivariate scatterplot (Figure \ref{fig:nontrafficeda}), `perc14_24` has the strongest correlation to `totfatrte`. Among the predictors, in general the traffic law variables correlate positively to each other. `bac08` and `bac10` have strong negative correlation, which is expected, since a state can have only one of these two laws in effect at a time. `unem` has a notable negative correlation to `vehicmillespc`, which also makes intuitive sense, since higher unemployment rate would mean less commuting for work. Another well-correlated pair of predictors are `vehicmilespc` and `sl70plus`, implying that people travel more for places with less restrictive speed limit. Although `perse` is well correlated to `vehicmilespc`, this may be a coincidence since perse laws were increasingly enforced, while per capita miles travels were also increasing in time. None of the predictor variables of interest have perfect correlation so the lack of perfect correlation assumption for linear regression is satisfied.

```{r fig.cap="\\label{fig:corrplot}Correlation Matrix of Predictor Variables and Fatality Rate", message = FALSE, warning = FALSE}
library(corrplot)
res2 <- cor(data[, c('totfatrte', 'bac08', 'bac10', 'perse', 'sbprim', 'sbsecon', 'sl70plus', 'gdl', 'perc14_24', 'unem', 'vehicmilespc')])
col<- colorRampPalette(c("blue", "white", "red"))(20)
corrplot(res2, type = 'lower', order = "hclust",  addCoef.col = "black", 
         tl.col = "black", tl.srt = 45, tl.cex=0.8, number.cex = 0.8)
```

# Part 2

2. (15%) How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

The *totfatrte* is defined as the total fatality per 100,000 people. The average in each year can be calculated as a simple average or weighted average after controlling for each state's population size, as shown in Table \ref{tab:avg_fatality}. In 1980, the simple and weighted annual average of `totfatrte` is 25.49 and 22.61, respectively. In comparison, in 2004, the simple and weighted annual average of `totfatrte` is 16.73 and 14.54. In general, the fatality rates are decreasing over time, and the weighted average is lower than simple average in the same year, because some of the more populated states have lower fatality rate. 

```{r}
# totfat averaged factoring population
weighted_avg <- data %>% group_by(year) %>% summarise(avg_totfatrte_weighted = sum(totfat)*100000/sum(statepop), .groups = 'drop')
year_avg$avg_totfatrte_weighted <- weighted_avg$avg_totfatrte_weighted

kable(year_avg, caption = "\\label{tab:avg_fatality}Yearly Average of Total Fatality Rate")
```

A pooled linear regression model was fitted using just the indicator variables for years. In the EDA we noted that `totfatrte` is strongly positively skewed, therefore we log transformed the variable. The follow is the truncated equation summarizing the result of the regression. See Table 2 in the appendix for complete table of coefficients.
$$\log(totfatrte) = 3.20 - 0.079d81 - 0.20d82 - 0.24d83 - 0.226d84 -0.24d85 \cdots -0.43d02 -0.44d03 - 0.45d04$$
This model gives us the time effects on total fatality rate. The intercept in this case is the logged average `totfatrte` across all states in 1980, the baseline year. Each of the coefficients `d81`, `d82`...`d04` is the average change in logged `totfatrte` relative to the base year 1980. The coefficients for the dummy variable for 1981 is not statistically significant at the 5% level, the rest are all highly significant. Using `d80` as the base level, all coefficients have a negative sign, implying the total fatality rate comparing to 1980 is lower for all years starting 1981. The magnitude of the coefficients are for most part increasing, meaning as time goes on, in general there's an increasingly larger negative difference in fatality rate comparing to 1980. 

While total fatality rate is decreasing over the 25 year period, it doesn't necessarily mean driving has become safer. Firstly, driving safety encompasses both fatality rate in accidents, as well as accident rates in general. This dataset does not capture overall accident rates, so it's possible that vehicular accident rates remained the same or even increased over time, but because the newer car models have better safety features, drivers are much less likely to be injured or killed in accidents and hence the drop in fatality rate. Additionally, because the fatality rate per fixed population rate, changes in demographics or lifestyle could indirectly lead to what appears to be decreasing fatality rate. For example, most major metropolises are growing in population size over time, and people living in the city tend to travel using means other than private vehicles. Along the same veins, in recent years, due to combination of improvement in public transit and environment advocacy, more people are shifting to public transportation. These people would be included in the denominator for traffic fatality rate, while not contributing much to the numerator, which would lower the total fatality rate as defined in this dataset.

```{r}
q2.lm <- lm(log(totfatrte)~d81+d82+d83+d84+d85+d86+d87+d88+d89+d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+d00+d01+d02+d03+d04, data=data)
q2.lm.se = sqrt(diag(vcovHC(q2.lm)))
```


# Part 3

3. (15%) Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)

For modeling purposes, we chose to not binarize the variables representing enactment of laws (`bac08`, `bac10`, `perse`, `sbprim`, `sbsecon`, `sl70plus`, `gdl`). From an intuitive perspective, if a law has effect on traffic fatality, implementing it middle of the year should result in that year's fatality rate averaging out to be somewhere in between if the law was in full effect the whole year and entirely not enacted the whole time. Binarizing the predictors would lose this meaningful relationship. By leaving the variables as decimals, the variables can be interpreted as the fraction of year when the law is effective, instead of an indicator variable representing simple presence or absence of a law.

As for the numerical variables, as seen in the EDA histogram in (Figure \ref{fig:nontrafficeda}), `perc14_24` is asymmetric but not strongly skewed, therefore a log transformation is not necessary. In contrast, `unem`, `vehicmilespc` as well as the outcome variable `totfatrte` were shown in the EDA to be obviously skewed in the positive direction, and `vehicmilespc` has increasing variance over time. This makes them good candidates for log transformation. Additionally, Shapiro-Wilk normality test shows that the linear model with untransformed variable would result in non-normal residuals (P = 9.96e-13), while residuals after transforming `vehicmilespc` and `totfatrte` is normally distributed (p = 0.14). Figure \ref{fig:q3untranslmdiag} displays the diagnostic plots of the untransformed linear model. Comparing to that of the transformed model (See Figure \ref{fig:q3lmdiag} for diagnostic models), the residuals of the untransformed linear model non-normal, heteroskedastic, and violates zero conditional mean. For these reasons, log-transformation of `totfatre`, `unem`, and `vehicmilespc` is preferable.

The following is the truncated equation summarizing the result of the regression. See Table 2 in the appendix for complete table of coefficients. 

\begin{align*}
\log(totfatrte) =& -11.246 -0.092d81 -0.29d82 \cdots -1.00d03 -0.98d04 -0.063bac08 -0.018bac10\\
&-0.020perse+0.0004sbprim + 0.020sbsecon + 0.232 sl70plus -0.027gdl + 0.017perc14\_24\\
&+0.26\log(unem) + 1.54\log(vehicmilespc)
\end{align*}

Breusch-Pagan test suggests the model using transformed variables still violates heteroskedascity. The coefficients for `bac10` , `perse`, `sbprim`, `sbsecon` and `gdl` are not significant when using heteroskedasticity robust standard error. The rest of the coefficients are significant at the 5% level. 
`bac8` and `bac10` indicate the proportion of the year when blood alcohol limit is at 0.08 and 0.1, respectively. Holding all other factors constant, in any given year, enforcing the legal BAC limit at 0.08 for the entire year is associated with about 6.3% decrease in fatality rate; enforcing the BAC limit at 0.1 is associated with 1.8% decrease in fatality rate, though this decrease is not statistically significantly different from zero. 

The signs for the coefficients of `perse` and `sbprim` are both negative, but neither is significant at 5% level, so even though the regression shows per se laws and primary seat belt law both have a negative effect on fatality rate, the effect may not be significantly different from zero.


```{r fig.cap="\\label{fig:q3untranslmdiag}Diagnostic Plots of the Pooled OLS Model Without Transformation"}
q3.untransformed <- lm(totfatrte~factor(year)+ bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+perc14_24+unem+vehicmilespc, data=data)

shapiro.test(residuals(q3.untransformed))

par(mfrow=c(2,2))

plot(q3.untransformed)
```

```{r fig.cap="\\label{fig:q3lmdiag}Diagnostic Plots of the Pooled OLS Model"}
q3.lm <- lm(log(totfatrte)~d81+d82+d83+d84+d85+d86+d87+d88+d89+d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+d00+d01+d02+d03+d04+
               bac08+bac10+perse+sbprim+sbsecon+sl70plus+gdl+perc14_24+log(unem)+log(vehicmilespc), data=data)

q3.lm.se = sqrt(diag(vcovHC(q3.lm)))
shapiro.test(residuals(q3.lm))
bptest(q3.lm)

par(mfrow=c(2,2))
plot(q3.lm)
```

# Part 4

4. (15%) Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

The fixed effect model takes the form

$$\ddot{y}_{it} = \beta_1\ddot{x}_{it1} + \beta_2\ddot{x}_{it2} \cdots + \beta_k\ddot{x}_{itk} + \ddot{u}_{it}, t = 1, 2, \cdots, T $$

See table in Table 2 in the appendix for complete table of coefficients with comparison to the model in Exercise 3. 

In the fixed effects model, the coefficients for `bac8`, `bac10`,  `sbprim`, `sbsecon` and `gdl` are not significant when using heteroskedasticity robust SE. The coefficient for `perc14_24` is marginally significant at the 10% level. The rest of the coefficients are significant at the 5% level. 

Comparing to the pooled OLS model, in the fixed effect model, the coefficient for `bac08` became smaller and non-significant; `perse` became highly significant with coefficient more than twice as large; `bac10` and `sbprim` remain non-significant as they are in the pooled OLS model.

According to the fixed effects model, the traffic laws that has the largest effects on traffic fatalities are the Graduated Driver License Law (Per Se Law) and higher speed limits. Hold all other factors constant, by enforcing the Per Se Law, traffic fatality rate is expected to decrease by approximate 5.9%. Conversely, by allowing a high speed limit (70mph or higher) or not enforcing speed limit at all, traffic fatality is expected to increase by 7.8%.

The most important drawback to using pooled OLS is that it assumes the unobserved effect $a_{i}$ is uncorrelated with any of the predictor variables at all times. If this assumption is not true or if the idiosyncratic error is correlated to the predictors, then pooled OLS is biased and inconsistent. Additionally, the pooled OLS estimates requires the six classical linear regression assumptions. CLM1 (linearity of parameters) is met by definition. It's unknown whether CLM2 (random sampling) is true due to the unknown sampling methods. CLM3 (no perfect linear relationship) was checked using the correlation matrix. CLM4 (Zero Conditional Mean or Exogeneity) was checked using diagnostic plots. The residual vs. fitted plot for the linear model after transformation shows the residuals to be roughly symmetrical around 0, with no strong patterns throughout the fitted values. CLM5 (homoskedascity) was checked using the Breusch–Pagan test, which suggests significant heteroskedascity. This was addressed by using heteroskedascity-robust standard errors. CLM6 (normality of residuals) was checked using the Shapiro-Wilk test and qq-plot, which suggests the model with transformation upholds the assumption.

Unlike the pooled OLS model, the fixed effect model allows for arbitrary correlation between the unobserved effect and the explanatory variables in any time period, and only requires the idiosyncratic error to be uncorrelated to the predictors. The other assumptions are similar to that of the pooled OLS model. QQ-plot of the residual shows rough normality (Figure \ref{fig:q4fediag}). The residual vs fitted plot  shows the idiosyncratic error to have approximately mean of zero, so exogeneity assumption is likely met. Absence of perfect linear relationship was confirmed through correlation matrix, and none of the explanatory variables included in the model are time invariant. The residuals for the fixed effect model is heteroskedastic according to Breusch–Pagan test (p = 1.81e-07). This issue is addressed by using heteroskedastic robust coefficients. Lastly, the model assumes no serial correlation between the idiosyncratic errors conditional on all explanatory variables and unobserved effects. This is the most problematic assumption in this specific model. The Breusch-Godfrey test suggested there is serial correlation in idiosyncratic errors (p <2.2e-16). In conjunction to the violation of heteroskedasticity, we could potentially address this by using heteroskedasticity and autocorrelation consistent standard error, or include lag terms in the regression to attempt to remove the serial correlation in the error.

Taking in account of the shortcomings of both models, the estimates from the Fixed-Effect model are more reliable. The assumption of no correlation between composite error ($a_i + u_{it}$) is unlikely to be true for the pool OLS to give unbiased estimates. Intuitive example of such violation is a state's geographical features and city layouts. The layout of cities in each state is mostly time invariant, and it would have a tangible impact on one of the predictors, the amount of traveling by vehicle per capita. Simultaneously, it would have a direct effect on vehicle accidental fatality rate. States with closely packed cities and narrow streets are likely to end up with higher accidental rate and higher fatalities.

```{r fig.width=8, fig.height=3, fig.cap="\\label{fig:q4fediag}Residual vs. Fitted and QQ-plot for Fixed Effects Model"}
panel_data <- pdata.frame(data, c("state", "year"))
q4.fe <- plm(log(totfatrte) ~ d81+d82+d83+d84+d85+d86+d87+d88+d89+d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+d00+d01+d02+d03+d04+
               bac08 + bac10 + perse + sbprim + sbsecon + sl70plus + gdl + perc14_24 + log(unem) + log(vehicmilespc), data = panel_data, model = "within")
q4.fe.se = sqrt(diag(vcovHC(q4.fe)))

bptest(q4.fe, studentize = F)
pbgtest(q4.fe)

par(mfrow=c(1,2 ))

Fitted.Values <- as.numeric(q4.fe$model[[1]] - q4.fe$residuals)
Residual <- as.numeric(q4.fe$residuals)

scatter.smooth(Fitted.Values, Residual, lpars = list(col = "red", lwd = 1, lty = 1))

qqPlot(Residual)
```

# Part 5

5. (10%) Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

The random effects model is reported in Table 2 in the appendix. 

The fixed effects model is more preferred over the random effects model. Hausman test also rejects the null-hypothesis that the random effect model is consistent (p = 1.68e-05). The main issue with the random effect model is that it assumes the unobserved effect given all explanatory variables is constant, that is, there is no correlation between the unobserved effect and the explanatory variables. As discussed previously, this assumption is unlikely to hold. On the other hand, because we are not using any time-invariant variables as predictors, fixed effect model can estimate the effects of all predictors on `totfatrte`. It's worth noting that all coefficients significant in the fixed effect model are also significant in the random effect model, all coefficients not significant in the FE model are also non-significant in RE model, and the signs of the coefficients remain the same between the two models.

```{r}
q5.re <-plm(log(totfatrte) ~ d81+d82+d83+d84+d85+d86+d87+d88+d89+d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+d00+d01+d02+d03+d04+
               bac08 + bac10 + perse + sbprim + sbsecon + sl70plus + gdl + perc14_24 + log(unem) + log(vehicmilespc), data = panel_data, model = "random")
q5.re.se = sqrt(diag(vcovHC(q5.re)))

phtest(q4.fe, q5.re)
```

# Part 6

6. (10%) Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

Due to the log transformation of `totfatrte` and `vehicmilespc`, there isn't a constant estimated effects from a raw increase of miles driven per capita. Holding all other factors constant, if `vehicmilespc` is originally 10,000 miles, increasing it by 1,000 miles is a 10% increase, then according to the Fixed Effect model's coefficient for `log(vehicmilespc)`, it would result in a 6.76% increase in total fatality rate. In contrast, if `vehicmilespc` was originally 5,000 or 20,000, increasing it by 1,000 miles would lead to 13.52% and 3.38% increase in `totfatrte`, respectively.

# Part 7

7. (5%) If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?

If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, the errors would be closer together and the standard errors are smaller than they should be. Consequently, the p-values obtained would be smaller than it should be and it would be easier to obtain a significant coefficient comparing to how significant it is in reality. One way to address this is to use clustering to obtain fully robust standard errors and test statistics. As discussed in part 4, the fixed effect model in this analysis has both serial correlation and heteroskedasticity in the residuals according to Breusch-Godfrey and Breusch-Pagan test, respectively. Without using heteroskedasticity and autocorrelation consistent standard errors, we should consider the possibility that the marginally significant coefficient (`perc14_24`) is in reality not significant.

# Appendix

Regression Model Results from Questions 2-5, using heteroskedastic robust standard error.

```{r message = FALSE, warning = FALSE, results = "asis"}
stargazer(q2.lm, q3.lm, q4.fe, q5.re, type = "latex", single.row = TRUE, font.size="small",
          se = list(q2.lm.se, q3.lm.se, q4.fe.se, q5.re.se), column.sep.width = "-15pt",
          column.labels = c("Pooled OLS 1", "Pooled OLS 2", "Fixed Effect", "Random Effect"),
          star.char = c("+", "*", "**", "***"), star.cutoffs = c(0.1, 0.05, 0.01, 0.001),
          notes = c("+ p<0.1; * p<0.05; ** p<0.01; *** p<0.001"), header = FALSE)
```