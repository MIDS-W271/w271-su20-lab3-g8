---
title: 'W271 Live Session 10: An Intro to Multivarite Time Series Analysis'
author: "Professor Jeffrey Yau"
output:
  pdf_document: default
  html_notebook: default
---

# Main topics covered in Week 10
    - Regression with multiple trending time series
    - Correlation of time series with trends
    - Spurious regression
    - Unit-root non-stationarity and Dickey-Fuller Test
    - Cointegration
    - Multivariate Time Series Models: Vector Autoregressive (VAR) model
        - Estimation, model diagnostics, model identification, model selection, assumption testing, and statistical inference / forecasting
    - Notion of cross-correlation


# Readings
**CM2009:** Paul S.P. Cowpertwait and Andrew V. Metcalfe. *Introductory Time Series with R*. Springer. 2009. 
  
  - Ch. 11

**HA:** Rob J Hyndman and George Athanasopoulos. Forecasting: Principles and Practice (https://otexts.org/fpp2/VAR.html):
  
  - Ch. 11.2


# Spurious Regression Example
```{r}
# Example from our text
set.seed(10)
x <- rnorm(100)
y <- rnorm(100)

for(i in 2:100) {
  x[i] <- x[i-1] + rnorm(1)
  y[i] <- y[i-1] + rnorm(1)
  }

plot.ts(x)
plot.ts(y)

plot(x, y)
cor(x, y)
summary(lm(y ~ x))
```


Some start-up codes:
```{r message=FALSE, warning=FALSE}
#sessionInfo()

# Insert the function to *tidy up* the code when they are printed out
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

# Clean up the workspace before we begin
rm(list = ls())

# Set working directory
# wd <- "~/Documents/Teach/Cal/w271/course-main-dev/live-session-files/week10"
# setwd(wd)

# Load libraries
library(car)
library(dplyr)
library(Hmisc)

library(forecast)
library(fpp2)
library(astsa)
library(xts)
library(vars)
library(zoo)
library(tseries)
```

Building a **VAR(p)** model also uses the iterative procedure in which the researchers conduct EDA, estimate the model, evaluate the estimated model, check assumptions, and, once a valid model is chosen, conduct statistical inference and forecasting.

Note that it is no longer that case that we can simply use the individual ACF and PACF of each of the componenet series alone to deterimine the order of a VAR(p) models. We, however, still should conduct EDA because we need to know how to set upthe VAR model in R.  

As AR-type model is only applicable to stationary series, if the series are already stationary, we can model them using a VAR to the data directly (known as a *“VAR in levels”*). If the series are non-stationary but not co-integrated, we take differences of the series in an attempt to transform them to stationary, then estimate a VAR model (known as a *“VAR in differences”*).

In what follow, we will estimate a series VAR model, using information criterion for the selection, a procedure already embedded in a function in the `vars()` library. We will also conduct model diagnostic and forecasting. We will use the *VARselect()* function to select the optimal model (based on information criterion) (again, for now).

A look at the `VAR()` fucntion.

```
VAR(y, p = 1, type = c("const", "trend", "both", "none"),
    season = NULL, exogen = NULL, lag.max = NULL,
    ic = c("AIC", "HQ", "SC", "FPE"))
    ## S3 method for class  varest 
    print(x, digits = max(3, getOption("digits") - 3), ...)
```

It mainly consists of three arguments: 
  - the data matrix object y (or an object that can be coerced to a matrix)
  - the integer lag-order p
  - the type of deterministic regressors to include in the VAR(p) model

An optimal lag-order can be determined based on an information criteria or the final prediction error of a VAR(p) with the function VARselect(). The arguments can be displayed using the `args()` function:

```{r}
args(VAR)
```

```{r}
args(VARselect)
```

# Example: Canadian Economy

This example takes the data that comes with `vars` library; it consists 
of 4 macroeconomic time series:

|----------|--------------------|
| e        | Employment         |
| prod     | Productivity       |
| rw       | Real Wage          |
| U        | unemployment rate  |

```{r}
data("Canada")
str(Canada)
head(Canada)
```

Since the data is alredy "cleaned" and is stored in a time series object, we can proceed to EDA

```{r}
plot.ts(Canada, main="4 Macro Time Series of the Canadian Economy")

# Alternvative, we can use autoplot
# for some reasons, embedded autoplot() within for-loop doesn't print the graph

for (k in 1:ncol(Canada)) {
  autoplot(Canada[,k])
}

tsplot <- function(series) {
  autoplot(series)
}
for (k in 1:ncol(Canada)) {
  #print(paste("Plot ", k))
  tsplot(Canada[,k])
}

par(mfrow=c(2,2))
autoplot(Canada[,1])
autoplot(Canada[,2])
autoplot(Canada[,3])
autoplot(Canada[,4])  

```

```{r}
# Scatterplot Matrix, which displays the contemporaneous correlation
scatterplotMatrix(~Canada[,1]+Canada[,2]+Canada[,3]+Canada [,4]);
  title("Contemporaneous Correlation of the 4 Macroeconomic Series ")
  
# Time series plot, ACF and PACF of each of the individual series

tsplot <- function(series, title) {
  par(mfrow=c(2,2)) 
  hist(series, main=""); title(title)
  plot.ts(series, main=""); title(title)
  acf(series, main=""); title(paste("ACF",title))  
  pacf(series, main=""); title(paste("ACF",title))    
}
tsplot(Canada[,1], "Employment")
tsplot(Canada[,2], "Productivity")
tsplot(Canada[,3], "Real Wage")
tsplot(Canada[,4], "Unmployment Rate")

# Correlation and Cross-correlation between the two series
par(mfrow=c(1,1))

corrfunc <- function(series1, series2) {
  cat("Correlation Matrix: ", cor(series1, series2))
  ccf(series1,series2) 
}

#corrfunc(Canada[,1],Canada[,2])

for (i in 1:4) {
  for (j in 1:4) {
    if (i != j & j > i) {
    corrfunc(Canada[,i],Canada[,j])
    }
  }
}

```

## Select optimal number of lags
```{r}
VARselect(Canada, lag.max = 8, type = "both")
```

The `VARselect()` enables the user to determine an optimal lag length according to an information criteria or the final prediction error of an empirical VAR(p)-process.

The R output above shows the lag length selected by each of the information criteria available in the `vars` package. There is a discrepancy between the VAR(3) selected by the AIC and the VAR(1) selected by the SC. In VAR modeling, we typically select model order using BIC. Note that both BIC (Bayesian Information Criterion) and SC (Schwarz Criterion) are referred to the same information criterion  developed by Gideon E. Schwarz in a paper title "Estimating the Dimension of a Model" published in *The Annals of Statistics* in a 1978.

In a next step, the VAR(1) is estimated with the function VAR() and as deterministic regressors a constant is included.

```{r}
var.fit1 <- VAR(Canada, p = 1, type = "both")
summary(var.fit1)
names(var.fit1)
```

```{r}
summary(var.fit1)
plot(var.fit1)
```

To check if the VAR(1) with a constant and a trend is a stable process, we will need to check if the moduli of the eigenvalues of the companion matrix are all less than one.

```{r}
roots(var.fit1)
```

# Diagnostic Testing
```{r}
# Test of normality:
var.fit1.norm <- normality.test(var.fit1, multivariate.only = TRUE)
names(var.fit1.norm)
var.fit1.norm

# Test of no serial correlation:
var.fit1.ptasy <- serial.test(var.fit1, lags.pt = 12, type = "PT.asymptotic")
var.fit1.ptasy
plot(var.fit1.ptasy)

# Test of the absence of ARCH effect:
var.fit1.arch <- arch.test(var.fit1)
names(var.fit1.arch)
var.fit1.arch
```

Forecast
```{r}
forecast(var.fit1) %>%
  autoplot() + xlab("Date")
```

# BREAKOUT SESSION

# Develop a Vector Autoregressive Model:

Use series `series01_liveSession_wk10.csv` and build a VAR model. You will have to examine the data, conduct EDA, use `VARselect` to choose a model that minimize `SC` (which is also `BIC`), estimate the chosen model, conduct residual diagnostic, test model assumptions, and make a 3-step ahead forecast.

```{r}
series.zoo <- "series01_liveSession_wk10.csv" %>% read.csv %>% zoo
str(series.zoo)
head(series.zoo)
summary(series.zoo)
```

## EDA

The plots of the 2 series are shown below.
These do appear to have some contemporaneous correlation.

```{r}
# two panel graph
autoplot(series.zoo, main = "Live Session 10 Data")

# one panel graph
autoplot(series.zoo, main = "Live Session 10 Data", facet=NULL)
```


The PACFs have no significant correlation after lag 1, while the ACFs have gradually declining correlation suggesting this might be an AR process.
The cross-correlation plot shows that there is meaningful cross correlation after several lags. 

```{r, fig.height=7}
layout(matrix(c(1, 2, 3, 4, 5, 5), 3, 2, byrow = TRUE))
acf(series.zoo$series1)
pacf(series.zoo$series1)
acf(series.zoo$series2)
pacf(series.zoo$series2)
ccf(series.zoo$series1, series.zoo$series2)
```

This plot shows the extent of the contemporaneous correlation.

```{r}
#with(series.zoo, cor(series1, series2))
scatterplotMatrix(series.zoo)
```

Use the Augmented Dickey-Fuller Test to test if each series has a unit root.
The p-value is too high to reject the null hypothesis of a unit root.
The Phillips-Ouliaris Cointegration Test is used to test the null hypothesis that the series are not cointegrated.
This can be rejected, so the series are cointegrated.

```{r, warning=FALSE}

adf.test(series.zoo$series1)
adf.test(series.zoo$series2)
po.test(series.zoo)
```

## Modeling

The `VARselect` function is used to compare models. All the metrics agree on the same order $p = 1$

```{r}
VARselect(series.zoo, lag.max = 10, type = "both")
```

We fit the model and examine the residuals. All of the auto-regressive coefficients are significant in this model. The constant and the trend terms are both not distinguishable from 0. This is not surprising, as there did not appear to be any trend from the EDA. Perhaps the `type = "both"` argument is unnecessary.

```{r}
series.fit <- VAR(series.zoo, p = 1, type = "both")
summary(series.fit)
```

The `serial.test` shows that the errors are not serially correlated. The ACF plot demonstrates a lack of autocorrelation in the errors. The Q-Q plot shows that the series' residuals are approximately normally distributed. 

```{r}
serial.test(series.fit)
series.fit %>% resid %>% acf
series.fit %>% resid %>% .[, "series1"] %>% qqPlot
series.fit %>% resid %>% .[, "series2"] %>% qqPlot
```

## Forecast

Making a prediction forecast 3 time steps into the future, the `fanchart` function from the vars package is used to visualize the confidence interval.

```{r}
series.fit %>% predict(n.ahead = 3, ci = 0.95) %>% fanchart()
```