---
title: 'W271 Live Session 10: An Intro to Multivarite Time Series Analysis'
author: "Professor Jeffrey Yau"
output:
  pdf_document: default
---

# Main topics covered in Week 10
    - Regression with multiple trending time series
    - Correlation of time series with trends
    - Spurious regression
    - Unit-root non-stationarity and Dickey-Fuller Test
    - Cointegration
    - Multivariate Time Series Models: Vector Autoregressive (VAR) model
        - Estimation, model diagnostics, model identification, model selection, assumption testing, and statistical inference / forecasting
    - Notion of cross-correlation


# Readings
**CM2009:** Paul S.P. Cowpertwait and Andrew V. Metcalfe. *Introductory Time Series with R*. Springer. 2009. 
  
  - Ch. 11

**HA:** Rob J Hyndman and George Athanasopoulos. Forecasting: Principles and Practice (https://otexts.org/fpp2/VAR.html):
  

# Spurious Regression Example

```{r}
# Example from our text
set.seed(10)
x <- rnorm(100)
y <- rnorm(100)

for(i in 2:100) {
  x[i] <- x[i-1] + rnorm(1)
  y[i] <- y[i-1] + rnorm(1)
  }

plot.ts(x)
plot.ts(y)

plot(x, y)
cor(x, y)
summary(lm(y ~ x))
```


Some start-up code:
```{r message=FALSE, warning=FALSE}
# Insert the function to *tidy up* the code when they are printed out
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

# Load libraries
library(car)
library(dplyr)
library(Hmisc)

library(forecast)
library(fpp2)
library(astsa)
library(xts)
library(vars)
```

Building a **VAR(p)** model also uses the iterative procedure in which the researchers conduct EDA, estimate the model, evaluate the estimated model, check assumptions, and, once a valid model is chosen, conduct statistical inference and forecasting.

Note that it is no longer that case that we can simply use the individual ACF and PACF of each of the componenet series alone to deterimine the order of a VAR(p) models. We, however, still should conduct EDA because we need to know how to set upthe VAR model in R.  

As AR-type model is only applicable to stationary series, if the series are already stationary, we can model them using a VAR to the data directly (known as a *“VAR in levels”*). If the series are non-stationary but not co-integrated, we take differences of the series in an attempt to transform them to stationary, then estimate a VAR model (known as a *“VAR in differences”*).

In what follow, we will estimate a series VAR model, using information criterion for the selection, a procedure already embedded in a function in the `vars()` library. We will also conduct model diagnostic and forecasting. We will use the *VARselect()* function to select the optimal model (based on information criterion) (again, for now).

A look at the `VAR()` fucntion.

```
VAR(y, p = 1, type = c("const", "trend", "both", "none"),
    season = NULL, exogen = NULL, lag.max = NULL,
    ic = c("AIC", "HQ", "SC", "FPE"))
    ## S3 method for class  varest 
    print(x, digits = max(3, getOption("digits") - 3), ...)
```

It mainly consists of three arguments: 
  - the data matrix object y (or an object that can be coerced to a matrix)
  - the integer lag-order p
  - the type of deterministic regressors to include in the VAR(p) model

An optimal lag-order can be determined based on an information criteria or the final prediction error of a VAR(p) with the function VARselect(). The arguments can be displayed using the `args()` function:

```{r}
args(VAR)
```

```{r}
args(VARselect)
```

# Example: Canadian Economy

This example takes the data that comes with `vars` library; it consists 
of 4 macroeconomic time series:

|----------|--------------------|
| e        | Employment         |
| prod     | Productivity       |
| rw       | Real Wage          |
| U        | unemployment rate  |

```{r}
data("Canada")
str(Canada)
head(Canada)
```

Since the data is alredy "cleaned" and is stored in a time series object, we can proceed to EDA

```{r}
plot.ts(Canada, main="4 Macro Time Series of the Canadian Economy")

# Alternvative, we can use autoplot
# for some reasons, embedded autoplot() within for-loop doesn't print the graph

for (k in 1:ncol(Canada)) {
  autoplot(Canada[,k])
}

tsplot <- function(series) {
  autoplot(series)
}
for (k in 1:ncol(Canada)) {
  #print(paste("Plot ", k))
  tsplot(Canada[,k])
}

par(mfrow=c(2,2))
autoplot(Canada[,1])
autoplot(Canada[,2])
autoplot(Canada[,3])
autoplot(Canada[,4])  

```

```{r}
# Scatterplot Matrix, which displays the contemporaneous correlation
scatterplotMatrix(~Canada[,1]+Canada[,2]+Canada[,3]+Canada [,4]);
  title("Contemporaneous Correlation of the 4 Macroeconomic Series ")
  
# Time series plot, ACF and PACF of each of the individual series

tsplot <- function(series, title) {
  par(mfrow=c(2,2)) 
  hist(series, main=""); title(title)
  plot.ts(series, main=""); title(title)
  acf(series, main=""); title(paste("ACF",title))  
  pacf(series, main=""); title(paste("ACF",title))    
}
tsplot(Canada[,1], "Employment")
tsplot(Canada[,2], "Productivity")
tsplot(Canada[,3], "Real Wage")
tsplot(Canada[,4], "Unmployment Rate")

# Correlation and Cross-correlation between the two series
par(mfrow=c(1,1))

corrfunc <- function(series1, series2) {
  cat("Correlation Matrix: ", cor(series1, series2))
  ccf(series1,series2) 
}

#corrfunc(Canada[,1],Canada[,2])

for (i in 1:4) {
  for (j in 1:4) {
    if (i != j & j > i) {
    corrfunc(Canada[,i],Canada[,j])
    }
  }
}

```

## Select optimal number of lags
```{r}
VARselect(Canada, lag.max = 8, type = "both")
```

The `VARselect()` enables the user to determine an optimal lag length according to an information criteria or the final prediction error of an empirical VAR(p)-process.

The R output above shows the lag length selected by each of the information criteria available in the `vars` package. There is a discrepancy between the VAR(3) selected by the AIC and the VAR(1) selected by the SC. In VAR modeling, we typically select model order using BIC. Note that both BIC (Bayesian Information Criterion) and SC (Schwarz Criterion) are referred to the same information criterion developed by Gideon E. Schwarz in a paper title "Estimating the Dimension of a Model" published in *The Annals of Statistics* in a 1978.

In a next step, the VAR(1) is estimated with the function VAR() and as deterministic regressors a constant is included.

```{r}
var.fit1 <- VAR(Canada, p = 1, type = "both")
summary(var.fit1)
names(var.fit1)
```

```{r}
summary(var.fit1)
plot(var.fit1)
```


# Diagnostic Testing

```{r}
# Test of normality:
var.fit1.norm <- normality.test(var.fit1, multivariate.only = TRUE)
names(var.fit1.norm)
var.fit1.norm

# Test of no serial correlation:
var.fit1.ptasy <- serial.test(var.fit1, lags.pt = 12, type = "PT.asymptotic")
var.fit1.ptasy
plot(var.fit1.ptasy)

# Test of the absence of ARCH effect:
var.fit1.arch <- arch.test(var.fit1)
names(var.fit1.arch)
var.fit1.arch
```

Forecast
```{r}
forecast(var.fit1) %>%
  autoplot() + xlab("Date")
```

# BREAKOUT SESSION

# Develop a Vector Autoregressive Model:

Use series `series01_liveSession_wk10.csv` and build a VAR model. You will have to examine the data, conduct EDA, use `VARselect` to choose a model that minimize `SC` (which is also `BIC`), estimate the chosen model, conduct residual diagnostic, test model assumptions, and make a 3-step ahead forecast.

```{r}
## Read in Data
df = read.csv('series01_liveSession_wk10.csv', header = T)
kable(cbind(head(df), tail(df)))
str(df)
kable(summary(df))
```

We see that our series has two columns, `series1` and `series2`. There does not seem to be any time values in the dataset. We note that there are `r sum(is.na(df))` missing values. 

Since there is no time given but we have 120 observations, I am using the default times for the time series object method; however, I will set the frequency to 12:

```{r}
series1 = ts(df[,1], frequency = 12)
series2 = ts(df[,2], frequency = 12)
df_ts = cbind(series1, series2)
```


Looking at `series1`:

```{r}
ggtsdisplay(df_ts[,1])
```

We see that the PACF decays after the first lag and the ACF has a gradual decay. The plot looks somewhat stationary; however, we might be able to transform it further by doing a first difference or log. 

Examining `series2`

```{r}
ggtsdisplay(df_ts[,2])
```

We see that the PACF has another spike at lag 3; however, it seems to be tailing off. The ACF gradually decays, but picks up again around lag 12 (which could be a sign of seasonality). We may explore transforming the series to achieve a stationary time series.

We see that the two plots spike and dip at around the same time, so let us run a scatter plot matrix to see the relationship between the two:

```{r}
scatterplotMatrix(~df_ts[,1]+df_ts[,2]);
  title("Contemporaneous Correlation of the 2 Series")
```

We see some relationship here, but let's examine the correlation explicitly:

```{r}
cor(df_ts[,1], df_ts[,2])
```

We see that while there is not a strong correlation, it is still substatial. 

Despite the EDA, we note from the in class lecture:

> Note that it is no longer that case that we can simply use the individual ACF and PACF of each of the componenet series alone to deterimine the order of a VAR(p) models. 
We now select the optimal number of lags:

```{r}
# YOUR CODE HERE
```

We see that VAR(1) minimizes the SC criteria, so we will estimate a VAR model with `p = 1`. 


```{r}
# YOUR CODE HERE
```

To check if the VAR(1) with a constant and a trend is a stable process, we will need to check if the moduli of the eigenvalues of the companion matrix are all less than one.

```{r}
# YOUR CODE HERE
```

We see that we are in good shape.

We now conduct diagnostic tests on our model:

We first conduct a normality test on our residuals, followed by a serial correlation test. The last test we conduct is an ARCH test to check if the squared residuals are a white noise process. 

```{r}
# Test of normality:
# YOUR CODE HERE

# Test of no serial correlation:
# YOUR CODE HERE

# Test of the absence of ARCH effect:
# YOUR CODE HERE
```

We see that we fail to reject the null that our residuals are normal for our normality test. Our Portmanteau Test also allows us to fail to reject the null that we have serial correlation. Finally, our Arch test rejects the null that the squared residuals are a white noise process. Despite failing this test, I still believe that our predictions will be ok; however, our prediction boundaries may be slightly off. 

We finally conduct a 3-step ahead forecast:

```{r}
# YOUR CODE HERE
```


