---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data: Live Session 8'
author: "Professor Jeffrey Yau"
output:
  pdf_document: default
---

# Main Topics Covered in Lecture 8:

    - Autoregressive (AR) models
    - Moving Average (MA) Models
    - Lag (or backshift) operators
    - Model simulation
    - Estimation, model diagnostics, model identification, model selection, assumption testing, and statistical inference

# Readings:

**CM2009:** Paul S.P. Cowpertwait and Andrew V. Metcalfe. *Introductory Time Series with R*. Springer. 2009. 

    - Ch. 3.1, 3.2, 4.5, 6.1 â€“ 6.4

**HA:** Rob J Hyndman and George Athanasopoulos. Forecasting: Principles and Practice (3rd edition).

    - 9.2, 9.3, 9.4


# Lecture Overview

In Lecture 7, we discussed applying classical regression to time series data, including the use of trend regression, seasonal-dummy regression, smoothing techniques, and exploratory data analysis for time series data, which requires the use of visuals displaying dynamics that are not visible under histogram or density plots, which don't capture the time element.

Classical linear regression models, however, are insufficient for explaining all of the interesting dynamics of a time series, meaning that there could be additional structure of the data that is not captured.

In this lecture, we will study in-depth autoregressive models (AR) and moving-average models (MA), both of which are the essential building blocks for the more general class of mixed autoregressive integrated moving average (ARIMA) models. We will learn about identification of the order of dependency in AR and MA models using ACF and PACF,estimation, diagonsis of residuals (after the model is estimated), model assumption testing, model performance evaluation, and forecasting. We will also discuss the *principle of parsimony* in building time-series models.
 
It is very important to keep in mind that this class of models applies only to **stationary processes**. Therefore, we always need to check for stationarity before applying AR(p) models to the data.

# Stationary Time Series Models

A stationary time series is one whose statistical moments are not functions of time. As such, time series with trends or seasonality are not stationary; the trend and seasonality will affect the value of the time series at different times. 

However, the stationarity requirement does not exclude the case of cyclic behavior that doesn't have trend or seasonality.  Cycles do not have a fixed length, so before the series is observed, the peaks and troughs of the cycles cannot be determined with certainty.

# An Introducion to the arima() function, Model Estimation, Model Selection, and Model Checking / Diagnostic Analysis

An Introducion to the `arima()` function with an excerpt from R documentation:

```
arima(x, order = c(0L, 0L, 0L),
      seasonal = list(order = c(0L, 0L, 0L), period = NA),
      xreg = NULL, include.mean = TRUE,
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c("CSS-ML", "ML", "CSS"), n.cond,
      SSinit = c("Gardner1980", "Rossignol2011"),
      optim.method = "BFGS",
      optim.control = list(), kappa = 1e6)

selected arguments:
x - a univariate time series

order - A specification of the non-seasonal part of the ARIMA model: the three integer components (p, d, q) are the AR order, the degree of differencing, and the MA order.

seasonal - A specification of the seasonal part of the ARIMA model, plus the period (which defaults to frequency(x)). This should be a list with components order and period, but a specification of just a numeric vector of length 3 will be turned into a suitable list with the specification as the order.

xreg - Optionally, a vector or matrix of external regressors, which must have the same number of rows as x.

include.mean - should the ARMA model include a mean/intercept term? The default is TRUE for undifferenced series, and it is ignored for ARIMA models with differencing.
```  

# ARIMA Modeling Procedure Recap

When fitting the class of ARIMA model to a set of time series data, the following procedure provides a useful general approach.

  1. Plot the data. Identify any unusual observations.

  2. If necessary, transform the data (e.g. using a Box-Cox transformation) to stabilize the variance.

  3. If the data are non-stationary: take first differences of the data until the data are stationary.

  4. Examine the ACF/PACF: Is an AR(p) or MA(q) model appropriate?
 
  5. Try your chosen model(s), and use appropriate metrics to choose a model.

  6. Model evaluation Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

  7. Once the residuals look like white noise, calculate forecasts.

# An Example      

```{r global_options, include=FALSE}
#knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
#                      echo=FALSE, warning=FALSE, message=FALSE)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE,warning=FALSE, message=FALSE)

# Load required libraries
library(car)
library(dplyr)
library(Hmisc)
library(ggplot2)
library(ggfortify)
library(plotly)
library(astsa)
library(fable)
library(fpp3)
```

**Breakout Sessions:**

  1. Load into R the given data series, *series1.csv*, a monthly series beginning in 2005 January. 

  2. Conduct a Time Series EDA

  3. Start by estimating a low-order AR or MA model
  
  4. Examine the residuals (after estimating the model)
  
  5. Discuss your findings

```{r}
df <- read.csv("series1.csv", header = FALSE, sep=",")

# Examine the data structure
str(df)
names(df)
head(df)
tail(df)

# Convert it into a time series object
series1 <-ts(df, start=c(2005,1), frequency = 12)
series1 <- as_tsibble(series1)
series1 <- tsibble(month = rep(yearmonth("2005 Jan") + 0:119), value = df$V1, index=month)
```

# Exploratory Time Series Data Analysis

```{r}
str(series1)
head(series1)
tail(series1)
autoplot(series1)

series1 %>% model(STL(value)) %>% components() %>% autoplot()
series1 %>% gg_tsdisplay(plot_type = "partial")
series1 %>% ggplot(aes(x = value)) + geom_histogram()
```

# Model Estimation and Diagnostics

```{r}
# choose an AR(3) model based on inspection of correlograms
series1_model <-  series1 %>% model(arima = ARIMA(value ~ pdq(3,0,0))) 

# model properties
series1_model %>% report(fit)

# residual characteristics
series1_model %>% gg_tsresiduals()

# test for autocorrelaton of residuals
augment(series1_model) %>% features(.resid, ljung_box)

# model roots (one real, two complex)
glance(series1_model)[['ar_roots']]

# inverse roots within unit circle
gg_arma(series1_model)

# modulus of roots exceed unity
Mod(polyroot(c(1, -coef(series1_model)[['estimate']])))
```

# Break-out Session

  1. Load `series2.csv`
  2. Convert it into a time series object
  3. Examine the basic structure of the series and conduct a time series EDA of the series
  4. Based on your time series EDA, does the series look more or less like a stationary series?  Does AR and/or MA model a good starting point?
  5. If your answer to question 6 is yes, then determine a good starting order and estimate an AR or a MA model. Try a couple other models with different orders.
  6. After model estimation, compare the models using your evaluation metrics
  7. Construct a 12-step ahead forecast


```{r}
# This example uses the forecast package rather than fable 
library(forecast)
library(gridExtra)
library(grid)

df <- read.csv("../series2.csv", header = FALSE, sep=",")

# Examine the data structure
str(df)
names(df)
head(df)
tail(df)

# Convert it into a time serie object
series2 <- ts(df$V1, start=c(2005,1), frequency = 12)

# Examine the converted data structure
str(series2)
head(cbind(time(series2),series2))

p1 = ggplot(series2, 
       aes(x=time(series2), y=series2)) + 
  geom_line(colour = "navy", size = 1) +
  ggtitle("A Given Montly Time Series") +
  theme(axis.title = element_text(size = rel(1.5)))

p2 = ggplot(df, aes(x=V1)) +
  geom_histogram(aes(fill = ..count..)) +
  ggtitle("A Given Monthly Time Series") +
  xlab("Series 1") + ylab("Frequency")

p3 = autoplot(acf(series2, plot = FALSE)) +
  ggtitle("ACF of A Given Monthly Time Series")

p4 = autoplot(pacf(series2, plot = FALSE)) +
  ggtitle("PACF of A Given Monthly Time Series")

grid.arrange(p1, p2, p3, p4, ncol=2)

# I started with AR(3) based on the patterns exhibited in acf and pacf
ma.fit1 <- arima(series2, c(0,0,3)) 
summary(ma.fit1)
str(ma.fit1)

# Model Selection -  Which model is selected based on the AIC?
auto.arima(series2)

# Model Evaluation (Residual Diagnostic)
summary(ma.fit1$resid)
str(ma.fit1$resid)
plot(ma.fit1$resid)

par(mfrow=c(3,2))
#plot(rnorm(120), type="l", main="Gaussian White Noise")
plot(ma.fit1$resid[-c(1:2)], type="l", main="Residuals: t-plot")
hist(ma.fit1$resid)
acf(ma.fit1$resid[-c(1:2)], main="ACF of the Residual Series")
pacf(ma.fit1$resid[-c(1:2)], main="ACF of the Residual Series")
qqPlot(ma.fit1$residuals)

# See https://www.rdocumentation.org/packages/WeightedPortTest/versions/1.0/topics/Weighted.Box.test
Box.test(residuals(ma.fit1), lag=24, type="Ljung")

tail(cbind(time(series2),series2)) 
ma.fit1.forecast = forecast(ma.fit1, h=12)
plot(ma.fit1.forecast)
```